{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../processed_data/reddit/processed_pro_qanon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(\"../processed_data/reddit/processed_anti_qanon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns=['Report Reasons', ' Removal Reasons', 'Permalink', 'Media Only', 'Media Title', 'Is Original Content', 'Is Crosspostable', 'Num Crossposts', 'Crosspost Parent List - Subdreddit(s)', 'View Count'])\n",
    "dff2 = dff.drop(columns=['Report Reasons', ' Removal Reasons', 'Permalink', 'Media Only', 'Media Title', 'Is Original Content', 'Is Crosspostable', 'Num Crossposts', 'Crosspost Parent List - Subdreddit(s)', 'View Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>URL</th>\n",
       "      <th>Upvote Ratio</th>\n",
       "      <th>Media Type</th>\n",
       "      <th>Media URL</th>\n",
       "      <th>Subreddit Subscribers</th>\n",
       "      <th>Author</th>\n",
       "      <th>Ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jcgco1</td>\n",
       "      <td>\"Why won't you denounce Qanon!?!?\" Trump: \"The...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'conspiracy'</td>\n",
       "      <td>https://i.redd.it/fgd78i2kbit51.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1395552</td>\n",
       "      <td>t2_ewwvg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jcgc9q</td>\n",
       "      <td>The Mueller Report: Page 2 \"the investigation ...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'conspiracy'</td>\n",
       "      <td>https://www.documentcloud.org/documents/595537...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1395552</td>\n",
       "      <td>t2_12dvhg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jcg77z</td>\n",
       "      <td>Sheldon Adelson just gave Trump another $75 mi...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'conspiracy'</td>\n",
       "      <td>https://www.politico.com/news/2020/10/15/adels...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1395552</td>\n",
       "      <td>t2_nxckpi4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jcfoau</td>\n",
       "      <td>Last night Trump was asked to denounce Q- a gr...</td>\n",
       "      <td>b'Submission statement: There is clear messagi...</td>\n",
       "      <td>b'conspiracy'</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/j...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1395552</td>\n",
       "      <td>t2_wnd5k</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jcfjni</td>\n",
       "      <td>Trump Calls Biden's Family - 'An Organized Cri...</td>\n",
       "      <td>b\"Just now at his address to seniors in Florid...</td>\n",
       "      <td>b'conspiracy'</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/j...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1395552</td>\n",
       "      <td>t2_tie34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              Title  \\\n",
       "0  jcgco1  \"Why won't you denounce Qanon!?!?\" Trump: \"The...   \n",
       "1  jcgc9q  The Mueller Report: Page 2 \"the investigation ...   \n",
       "2  jcg77z  Sheldon Adelson just gave Trump another $75 mi...   \n",
       "3  jcfoau  Last night Trump was asked to denounce Q- a gr...   \n",
       "4  jcfjni  Trump Calls Biden's Family - 'An Organized Cri...   \n",
       "\n",
       "                                                Text      Subreddit  \\\n",
       "0                                             b'nan'  b'conspiracy'   \n",
       "1                                             b'nan'  b'conspiracy'   \n",
       "2                                             b'nan'  b'conspiracy'   \n",
       "3  b'Submission statement: There is clear messagi...  b'conspiracy'   \n",
       "4  b\"Just now at his address to seniors in Florid...  b'conspiracy'   \n",
       "\n",
       "                                                 URL  Upvote Ratio Media Type  \\\n",
       "0                https://i.redd.it/fgd78i2kbit51.jpg          1.00        NaN   \n",
       "1  https://www.documentcloud.org/documents/595537...          0.33        NaN   \n",
       "2  https://www.politico.com/news/2020/10/15/adels...          0.88        NaN   \n",
       "3  https://www.reddit.com/r/conspiracy/comments/j...          0.66        NaN   \n",
       "4  https://www.reddit.com/r/conspiracy/comments/j...          0.57        NaN   \n",
       "\n",
       "  Media URL  Subreddit Subscribers      Author  Ups  \n",
       "0       NaN                1395552    t2_ewwvg    3  \n",
       "1       NaN                1395552   t2_12dvhg    0  \n",
       "2       NaN                1395552  t2_nxckpi4    6  \n",
       "3       NaN                1395552    t2_wnd5k   11  \n",
       "4       NaN                1395552    t2_tie34    3  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>URL</th>\n",
       "      <th>Upvote Ratio</th>\n",
       "      <th>Media Type</th>\n",
       "      <th>Media URL</th>\n",
       "      <th>Subreddit Subscribers</th>\n",
       "      <th>Author</th>\n",
       "      <th>Ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>je1pju</td>\n",
       "      <td>According to the Holy Messiah, Biden is now si...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'Qult_Headquarters'</td>\n",
       "      <td>https://i.redd.it/2xcfi8fo02u51.jpg</td>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32634</td>\n",
       "      <td>t2_wtacu</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>je0dgj</td>\n",
       "      <td>QAnon has hijacked the name of a real organiza...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'Qult_Headquarters'</td>\n",
       "      <td>https://www.cnn.com/videos/tech/2020/10/19/sav...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32634</td>\n",
       "      <td>t2_fojuj</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jdwy2c</td>\n",
       "      <td>Loose, superficial mainstream take on a \"key p...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'Qult_Headquarters'</td>\n",
       "      <td>https://heavy.com/news/thomas-schoenberger-qanon/</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32634</td>\n",
       "      <td>t2_10dzl0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jdw688</td>\n",
       "      <td>Obviously fake quotes from Qanon mascot Mel Gi...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'Qult_Headquarters'</td>\n",
       "      <td>https://www.reddit.com/gallery/jdw688</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32634</td>\n",
       "      <td>t2_7qujgkou</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jdv15k</td>\n",
       "      <td>Uh oh, just a glitch. Interesting who picked u...</td>\n",
       "      <td>b'nan'</td>\n",
       "      <td>b'Qult_Headquarters'</td>\n",
       "      <td>https://krebsonsecurity.com/2020/10/qanon-8cha...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32634</td>\n",
       "      <td>t2_q48st</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              Title    Text  \\\n",
       "0  je1pju  According to the Holy Messiah, Biden is now si...  b'nan'   \n",
       "1  je0dgj  QAnon has hijacked the name of a real organiza...  b'nan'   \n",
       "2  jdwy2c  Loose, superficial mainstream take on a \"key p...  b'nan'   \n",
       "3  jdw688  Obviously fake quotes from Qanon mascot Mel Gi...  b'nan'   \n",
       "4  jdv15k  Uh oh, just a glitch. Interesting who picked u...  b'nan'   \n",
       "\n",
       "              Subreddit                                                URL  \\\n",
       "0  b'Qult_Headquarters'                https://i.redd.it/2xcfi8fo02u51.jpg   \n",
       "1  b'Qult_Headquarters'  https://www.cnn.com/videos/tech/2020/10/19/sav...   \n",
       "2  b'Qult_Headquarters'  https://heavy.com/news/thomas-schoenberger-qanon/   \n",
       "3  b'Qult_Headquarters'              https://www.reddit.com/gallery/jdw688   \n",
       "4  b'Qult_Headquarters'  https://krebsonsecurity.com/2020/10/qanon-8cha...   \n",
       "\n",
       "   Upvote Ratio Media Type Media URL  Subreddit Subscribers       Author  Ups  \n",
       "0          0.98        NaN       NaN                  32634     t2_wtacu   71  \n",
       "1          0.78        NaN       NaN                  32634     t2_fojuj    5  \n",
       "2          0.89        NaN       NaN                  32634    t2_10dzl0   14  \n",
       "3          0.92        NaN       NaN                  32634  t2_7qujgkou   31  \n",
       "4          1.00        NaN       NaN                  32634     t2_q48st   24  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Based on an in-lab implementation.\n",
    "with efficiency update.\n",
    "\"\"\"\n",
    "# Author: Zixiaofan (Brenda) Yang, brenda@cs.columbia.edu.\n",
    "# Please contact me if you have any questions\n",
    "# The LIWC dictionary file is only distributed to users who bought the original software, so please don't distribute it outside speech lab, thanks!\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from typing import List, Text, Optional, Dict\n",
    "\n",
    "\n",
    "# we separate this out into a independent directory.\n",
    "LIWC_PATH = './LIWC2015_English.dic'\n",
    "\n",
    "\n",
    "def get_all_categories(dic_file: Text = LIWC_PATH) -> List[Text]:\n",
    "    \"\"\"\n",
    "    Get all category names in the LIWC dictionray\n",
    "\n",
    "    Output:\n",
    "        cat_list: all categories in the dictionary\n",
    "\n",
    "    Parameters:\n",
    "        dic_file: a LIWC style dictionary file\n",
    "            English: ./LIWC2015_English.dic\n",
    "            Chinese: ./Simplified_Chinese_LIWC2015_Dictionary.dic\n",
    "    \"\"\"\n",
    "    liwc_dic = open(dic_file, 'r', encoding=\"utf-8\")\n",
    "    phase = 0\n",
    "    cat_list = []\n",
    "    for line in liwc_dic:\n",
    "        line = line.strip()\n",
    "        if '%' in line:\n",
    "            phase += 1\n",
    "            continue\n",
    "        if phase == 1:  # category phase\n",
    "            while '\\t' in line:\n",
    "                line = line.replace('\\t', ' ')\n",
    "            while '  ' in line:\n",
    "                line = line.replace('  ', ' ')\n",
    "            if line.split(' ')[0].isdigit():\n",
    "                cat_list.append(line.split(' ')[1])\n",
    "        if phase == 2:\n",
    "            break\n",
    "    return cat_list\n",
    "\n",
    "\n",
    "def get_words_in_category(category: Text,\n",
    "                          dic_file: Text = LIWC_PATH):\n",
    "    \"\"\"\n",
    "    Get all words in dictionary for a certain LIWC category\n",
    "\n",
    "    Output:\n",
    "        word_list: all words in the category\n",
    "\n",
    "    Parameters:\n",
    "        dic_file: a LIWC style dictionary file\n",
    "            English: ./LIWC2015_English.dic\n",
    "            Chinese: ./Simplified_Chinese_LIWC2015_Dictionary.dic\n",
    "        category: a LIWC category name\n",
    "    \"\"\"\n",
    "    liwc_dic = open(dic_file, 'r', encoding=\"utf-8\")\n",
    "    phase = 0\n",
    "    word_list = []\n",
    "    index = None\n",
    "    for line in liwc_dic:\n",
    "        line = line.strip()\n",
    "        if '%' in line:\n",
    "            phase += 1\n",
    "            if phase == 2 and index is None:\n",
    "                print('cannot find this liwc category')\n",
    "                return []\n",
    "            continue\n",
    "        if phase == 1:  # category phase\n",
    "            while '\\t' in line:\n",
    "                line = line.replace('\\t', ' ')\n",
    "            while '  ' in line:\n",
    "                line = line.replace('  ', ' ')\n",
    "            if line.split(' ')[1] == category:\n",
    "                index = line.split(' ')[0]\n",
    "        if phase == 2:  # word phase\n",
    "            while '\\t' in line:\n",
    "                line = line.replace('\\t', ' ')\n",
    "            while '  ' in line:\n",
    "                line = line.replace('  ', ' ')\n",
    "            fields = line.split(' ')\n",
    "            if index in fields:\n",
    "                word_list.append(fields[0])\n",
    "    # print('found words: ', word_list)\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def liwc_categories_in_data(categories: List[Text], liwc_list: List[Text],\n",
    "                            word_with_star: Optional[bool] = True,\n",
    "                            normalize: Optional[bool] = True) -> List[Dict[Text, float]]:\n",
    "    \"\"\"A more efficient implementation of the liwc feature extraction\n",
    "    process, where we can run multiple categories on multiple input,\n",
    "    only once.\n",
    "\n",
    "    categories: --- category specification to be calculated.\n",
    "    liwc_list: --- text list that is to be analysed, for each line of text an individual liwc feature is calculated.\n",
    "    word_with_star: --- whether taking wild-card matching into account\n",
    "    normalize: --- whether to normalize the LIWC features to [0, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    cat_word_lists = {}\n",
    "    for category in categories:\n",
    "        word_list = get_words_in_category(category)\n",
    "        #  elif isinstance(category, list):\n",
    "        #      word_list = category\n",
    "\n",
    "        affix_word_list = []\n",
    "        if word_with_star:\n",
    "            original_word_list = word_list[:]\n",
    "            word_list = []\n",
    "            for word in original_word_list:\n",
    "                if '*' in word:\n",
    "                    affix_word_list.append(word)\n",
    "                else:\n",
    "                    word_list.append(word)\n",
    "\n",
    "        cat_word_lists[category] = (word_list, affix_word_list)\n",
    "\n",
    "    #  lines = liwc_file.readlines()[start_row:]\n",
    "    result = []\n",
    "    for line in liwc_list:\n",
    "        word_count = defaultdict(int)\n",
    "        #  line = line.strip().split(',')[text_column].lower()\n",
    "        # TODO: adding more regularization if needed\n",
    "        text = line.lower().split(' ')\n",
    "        for wt in text:\n",
    "            word_count[wt] += 1\n",
    "\n",
    "        sum_len = sum([word_count[wt] for wt in word_count])\n",
    "        cat_feat = {}\n",
    "\n",
    "        for category in categories:\n",
    "            word_list, affix_word_list = cat_word_lists[category]\n",
    "            c_count = 0\n",
    "            for w in word_list:\n",
    "                if w in word_count:\n",
    "                    c_count += word_count[w]\n",
    "\n",
    "            for w in affix_word_list:\n",
    "                for text_w in word_list:\n",
    "                    if text_w[:len(w)-1] == w[:-1]:\n",
    "                        c_count += word_count[text_w]\n",
    "\n",
    "            cat_feat[category] = c_count\n",
    "\n",
    "        result.append({cat: cat_feat[cat] / (sum_len if normalize else 1) for cat in categories})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "Given a list of words, compute LIWC features for both word-level and category-level, including\n",
    " (1) sentence length (#words in the text)\n",
    " (2) score for each of the word in the input word list - LIWC software doesn't have these word-level scores\n",
    " (3) treat the input word list as a LIWC category, compute a LIWC score for the category\n",
    "\n",
    "Outputs:\n",
    "    liwc_feat: LIWC features\n",
    "    liwc_names: the name of the features ('length'/word/'all_words')\n",
    "    file_list: the names of the files (rows) being analyzed\n",
    "\n",
    "Parameters:\n",
    "    word_list: a list of words (all words are from one single category)\n",
    "    liwc_file_name:  a csv file with texts in one column\n",
    "    text_column: the column index (start from 0) containing texts to analyze\n",
    "    name_column: the column index (start from 0) containing the names of the text (e.g. wav file names)\n",
    "    start_row: the row index(start from 0) to start analysis, used to skip headers\n",
    "    word_with_star: whether to consider affix words with star (set True for English, False for Chinese)\n",
    "'''\n",
    "\n",
    "\n",
    "def compute_customized_liwc_feature(word_list, liwc_file_name, start_row=2,\n",
    "                                    word_with_star=True,\n",
    "                                    text_column=4, name_column=0):\n",
    "\n",
    "    if len(set(word_list)) != len(word_list):\n",
    "        print('Found duplications in the provided word list, removing duplications')\n",
    "        word_list = list(set(word_list))\n",
    "\n",
    "    affix_word_list = []\n",
    "    if word_with_star:\n",
    "        original_word_list = word_list[:]\n",
    "        word_list = []\n",
    "        for word in original_word_list:\n",
    "            if '*' in word:\n",
    "                affix_word_list.append(word)\n",
    "            else:\n",
    "                word_list.append(word)\n",
    "    print('# words:', len(word_list), '; # affix words: ', len(affix_word_list))\n",
    "\n",
    "    liwc_names = ['length']+word_list[:]+affix_word_list[:]+['all_words']\n",
    "\n",
    "    # compute liwc score\n",
    "    liwc_file = open(liwc_file_name, 'r')\n",
    "    lines = liwc_file.readlines()\n",
    "    liwc_feat = []\n",
    "    file_list = []\n",
    "    for line in lines[start_row:]:\n",
    "        line = line.strip().split(',')\n",
    "\n",
    "        file_list.append(line[name_column])\n",
    "\n",
    "        text = line[text_column].strip().lower().split(' ')\n",
    "        length = len(text)\n",
    "        feat = []\n",
    "        feat.append(length)\n",
    "        for w in word_list:\n",
    "            c = text.count(w)\n",
    "            feat.append(float(c)/length)\n",
    "\n",
    "        for w in affix_word_list:\n",
    "            assert(w[-1] == '*' and '*' not in w[:-1])\n",
    "            c = 0\n",
    "            for text_w in text:\n",
    "                if text_w[:len(w)-1] == w[:-1]:\n",
    "                    c += 1\n",
    "            feat.append(float(c)/length)\n",
    "        feat.append(sum(feat[1:]))\n",
    "        liwc_feat.append(feat)\n",
    "\n",
    "    liwc_file.close()\n",
    "\n",
    "    return liwc_feat, liwc_names, file_list\n",
    "\n",
    "\n",
    "'''\n",
    "Significance test using Pearson correlation. Can do either category-level or word-level.\n",
    "\n",
    "Parameters:\n",
    "    liwc_feat: LIWC features\n",
    "    liwc_names: the name of the features ('length'/word/'all_words')\n",
    "    scores: a list of scores or labels (e.g. charisma ratings)\n",
    "    only_output_all: If Ture, treat all the words as one category; if False, do significance test on each word\n",
    "'''\n",
    "\n",
    "\n",
    "def significance_test(liwc_feat, liwc_names, scores, only_output_all=False):\n",
    "\n",
    "    if len(scores) != len(liwc_feat) or len(liwc_names) != len(liwc_feat[0]):\n",
    "        print('input dimension wrong, please check')\n",
    "        return\n",
    "    if liwc_names[-1] != 'all_words':\n",
    "        print('the last liwc feature should be all_words, please check')\n",
    "        return\n",
    "\n",
    "    liwc_feat = np.asarray(liwc_feat)\n",
    "    corr = []\n",
    "    count = 0\n",
    "    for i in range(len(liwc_names)):\n",
    "        feats = liwc_feat[:, i]\n",
    "        r, p = scipy.stats.pearsonr(scores, feats)\n",
    "        corr.append((r, p))\n",
    "        if not np.isnan(r) and abs(p) < 0.05:\n",
    "            count += 1\n",
    "    print(count, ' out of ', len(liwc_names), ' are significant on 0.05 level')\n",
    "\n",
    "    if only_output_all:\n",
    "        if corr[-1][1] < 0.05:\n",
    "            print('all words as one category - significant! ', corr[-1])\n",
    "        else:\n",
    "            print('all words as one category - not significant')\n",
    "    else:\n",
    "        importance_tuple = list(zip(liwc_names, corr))\n",
    "        importance_tuple = sorted(importance_tuple,\n",
    "                                  key=lambda i: i[1][1], reverse=False)\n",
    "        sig = [t for t in importance_tuple if t[1][1] < 0.05]\n",
    "        print('significant words are: ', sig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
